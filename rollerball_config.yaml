#Указывает какое поведение отслеживается
behaviors:
  #Имя модели поведения
  RollerBall:
    #Тип обучения, по умолчанию ppo
    trainer_type: ppo
    #Параметры содержащие данные которые управляют обучением
    hyperparameters:
    #Количество опытов в каждой итерации градиентного спуска.
      batch_size: 10
      #При trainer_type:
      # PPO: количество опытов, которые необходимо собрать перед обновлением модели поведения. Соответствует тому, сколько опыта должно быть собрано, прежде чем мы будем изучать или обновлять модель. Это значение должно быть в несколько раз больше, чем batch_size. Обычно больший размер буфера соответствует более стабильным обновлениям обучения.
      # SAC: максимальный размер буфера опыта — примерно в тысячи раз больше, чем ваши эпизоды, чтобы SAC мог учиться как на старом, так и на новом опыте.
      buffer_size: 100
      # Начальная скорость обучения для градиентного спуска
      learning_rate: 3.0e-4
      #Сила энтропийной регуляризации, которая делает поведение «более случайным».
      beta: 5.0e-4
      #Влияет на то, насколько быстро поведение может развиваться во время обучения
      epsilon: 0.2
      # Параметр регуляризации (лямбда), используемый при расчете обобщенной оценки преимущества. Его можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при расчете обновленной оценки стоимости.
      lambd: 0.99
      # Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска.
      num_epoch: 3
      #Определяет, как скорость обучения изменяется с течением времени.
      learning_rate_schedule: linear
    #Конфигурация для нейронной сети
    network_settings:
      # Применяется ли нормализация к входным данным векторного наблюдения.
      normalize: false
      #Количество юнитов в скрытых слоях нейронной сети
      hidden_units: 128
      #Количество скрытых слоев в нейронной сети
      num_layers: 2
    #Позволяет задавать настройки как для внешних (т. е. основанных на среде), так и для внутренних сигналов вознаграждения (например, любопытство и GAIL).
    reward_signals:
      #Настройки для сигналов основанных на среде
      extrinsic:
        #Коэффициент дисконтирования для будущих вознаграждений, поступающих из окружающей среды.
        gamma: 0.99
        #Фактор, на который умножается вознаграждение, данное средой. Типичные диапазоны будут варьироваться в зависимости от сигнала вознаграждения.
        strength: 1.0
    #Максимальное кол-во шагов
    max_steps: 500000
    #Сколько шагов опыта нужно собрать для каждого агента, прежде чем добавить его в буфер опыта.
    time_horizon: 64
    #Количество опытов, которое необходимо собрать перед созданием и отображением статистики обучения.
    summary_freq: 10000